import feedparser
import os
import json
import ollama
import logging
from datetime import datetime, timedelta
from time import mktime
from typing import Set, Optional
from pathlib import Path

from config import ProjectConfig, logger
from ai_research import WebResearcher
from obsidian_manager import ObsidianGardener

class NewsAgent:
    """
    Automates fetching and summarizing cybersec news.
    Now with integrated Obsidian linking and DORA tagging.
    """
    
    RSS_FEEDS = {
        "Sekurak": "https://feeds.feedburner.com/sekurak",
        "Niebezpiecznik": "https://feeds.feedburner.com/niebezpiecznik",
        "Zaufana Trzecia Strona": "https://zaufanatrzeciastrona.pl/feed/"
    }

    def __init__(self):
        self.news_dir = ProjectConfig.OBSIDIAN_VAULT / "Newsy"
        self.news_dir.mkdir(parents=True, exist_ok=True)
        self.history_file = ProjectConfig.BASE_DIR / "processed_news.json"
        self.model = ProjectConfig.OLLAMA_MODEL
        self.researcher = WebResearcher()

    def _load_history(self) -> Set[str]:
        if self.history_file.exists():
            try:
                return set(json.loads(self.history_file.read_text()))
            except:
                return set()
        return set()

    def _save_history(self, history: Set[str]):
        self.history_file.write_text(json.dumps(list(history)))

    def run(self, limit: int = 5) -> int:
        history = self._load_history()
        count = 0
        
        for source, url in self.RSS_FEEDS.items():
            logger.info(f"Checking feed: {source}", extra={"tags": "NEWS-SYNC"})
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries[:limit]:
                    if entry.link in history: continue
                    
                    # Basic analysis
                    summary = self._analyze(entry.link, entry.title)
                    if summary:
                        path = self._save_note(entry.title, entry.link, summary, source)
                        history.add(entry.link)
                        count += 1
            except Exception as e:
                logger.error(f"Feed error ({source}): {e}")
        
        self._save_history(history)
        return count

    def _analyze(self, url: str, title: str) -> Optional[str]:
        _, content = self.researcher.fetch_article_content(url)
        if not content: return None

        prompt = "Jesteś analitykiem cybersec. Podsumuj ten news w 5 konkretnych zdaniach technicznych."
        try:
            resp = ollama.chat(model=self.model, messages=[
                {'role': 'system', 'content': prompt},
                {'role': 'user', 'content': f"Tytuł: {title}\n\nTreść: {content[:8000]}"}
            ])
            return resp['message']['content']
        except Exception:
            return None

    def _save_note(self, title: str, url: str, summary: str, source: str) -> Path:
        date_str = datetime.now().strftime("%Y-%m-%d")
        safe_title = self.researcher.clean_filename(title)[:50]
        filename = f"{date_str}-news-{safe_title}.md"
        path = self.news_dir / filename
        
        content = f"""
---
created: {datetime.now().strftime("%Y-%m-%d %H:%M")}
tags: [news, cybersec, {source.lower()}]
source: {url}
---

# {title}

**Source:** {source} | [Original]({url})

## Podsumowanie AI
{summary}

---
*Generated by NewsAgent*"""
        path.write_text(content, encoding='utf-8')
        
        # Link concepts
        gardener = ObsidianGardener()
        gardener.process_file(str(path))
        return path
