import feedparser
import os
import json
import ollama
from datetime import datetime, timedelta
from time import mktime
from ai_research import fetch_article_content, clean_filename

# --- KONFIGURACJA ---
VAULT_PATH = "/mnt/c/Users/marci/Documents/Obsidian Vault/Education"
NEWS_DIR = os.path.join(VAULT_PATH, "Newsy")
HISTORY_FILE = "processed_news.json"
MODEL = "bielik"  # Możesz zmienić na deepseek-r1
DAYS_LOOKBACK = 3

RSS_FEEDS = {
    "Sekurak": "https://feeds.feedburner.com/sekurak",
    "Niebezpiecznik": "https://feeds.feedburner.com/niebezpiecznik",
    "Zaufana Trzecia Strona": "https://zaufanatrzeciastrona.pl/feed/",
    "ZTS - Weekendowa": "https://zaufanatrzeciastrona.pl/tag/weekendowa-lektura/feed/"
}

SUMMARY_PROMPT = """
Jesteś ekspertem cyberbezpieczeństwa. Twoim zadaniem jest przeczytanie artykułu i stworzenie SKONDENSOWANEGO podsumowania.

WYMAGANIA:
1. Podsumowanie musi mieć DOKŁADNIE 5 ZDAŃ.
2. Skup się tylko na kluczowych informacjach: co się stało, jakie jest zagrożenie, jak się chronić.
3. Jeśli artykuł jest o podatności (CVE), podaj numer CVE w jednym ze zdań.
4. Język polski, styl profesjonalny i konkretny.
"""

def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r") as f:
            return set(json.load(f))
    return set()

def save_history(history_set):
    with open(HISTORY_FILE, "w") as f:
        json.dump(list(history_set), f)

def analyze_article(url, title):
    print(f"[*] Pobieranie: {title}")
    _, content = fetch_article_content(url)
    
    if not content:
        return None

    # Ograniczenie treści dla szybkości (pierwsze 10k znaków zazwyczaj wystarcza dla newsa)
    content = content[:10000]

    try:
        response = ollama.chat(model=MODEL, messages=[
            {'role': 'system', 'content': SUMMARY_PROMPT},
            {'role': 'user', 'content': f"Tytuł: {title}\n\nTreść:\n{content}"}
        ])
        return response['message']['content']
    except Exception as e:
        print(f"[!] Błąd AI: {e}")
        return None

def save_news_note(title, url, summary, source_name):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    date_prefix = datetime.now().strftime("%Y-%m-%d")
    
    safe_title = clean_filename(title)[:50]
    filename = f"{date_prefix}-news-{safe_title}.md"
    filepath = os.path.join(NEWS_DIR, filename)
    
    note_content = f"""
---
created: {timestamp}
tags:
  - news
  - cybersec
  - {clean_filename(source_name)}
source: {url}
---

# {title}

**Źródło:** {source_name} | [Oryginał]({url})

## Podsumowanie AI (5 zdań)

{summary}

---
*Auto-generated by News Agent*
"""

    os.makedirs(NEWS_DIR, exist_ok=True)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(note_content)
    return filepath

def is_recent(entry, days=DAYS_LOOKBACK):
    """Sprawdza, czy artykuł jest nowszy niż X dni."""
    if not hasattr(entry, 'published_parsed'):
        return True # Jeśli brak daty, zakładamy, że jest świeży (bezpiecznik)
    
    published_time = datetime.fromtimestamp(mktime(entry.published_parsed))
    threshold = datetime.now() - timedelta(days=days)
    return published_time > threshold

def run_news_automator(limit_per_feed=20):
    """Uruchamia proces pobierania i analizy newsów."""
    history = load_history()
    new_articles_count = 0
    
    print(f"[*] Uruchamianie Agenta Newsowego (Ostatnie {DAYS_LOOKBACK} dni).")
    
    for name, url in RSS_FEEDS.items():
        print(f"\n--- Sprawdzanie: {name} ---")
        feed = feedparser.parse(url)
        
        # Sprawdzamy wpisy
        for entry in feed.entries[:limit_per_feed]:
            link = entry.link
            title = entry.title
            
            # 1. Sprawdzenie daty
            if not is_recent(entry):
                # Jeśli wpis jest stary, to prawdopodobnie kolejne też będą stare (RSS jest chronologiczny),
                # ale dla pewności po prostu go pomijamy.
                continue

            # 2. Sprawdzenie historii
            if link in history:
                print(f"[SKIP] Już przetworzono: {title}")
                continue
            
            print(f"[NEW] Przetwarzanie: {title}")
            summary = analyze_article(link, title)
            
            if summary:
                save_news_note(title, link, summary, name)
                history.add(link)
                save_history(history) # Zapisujemy po każdym sukcesie
                new_articles_count += 1
                print(f"[OK] Zapisano notatkę.")
            else:
                print("[FAIL] Nie udało się wygenerować podsumowania.")
    
    return new_articles_count

if __name__ == "__main__":
    count = run_news_automator()
    print(f"\n[KONIEC] Przetworzono {count} nowych artykułów.")
