import os
import argparse
import requests
from bs4 import BeautifulSoup
import ollama
import re
import logging
from datetime import datetime
from tqdm import tqdm
from typing import List, Tuple, Optional
from pathlib import Path

from config import ProjectConfig, logger
from obsidian_manager import ObsidianGardener

class WebResearcher:
    """
    Fetches and analyzes web articles using local AI.
    Integrated with ProjectConfig and ObsidianGardener.
    """
    
    SYSTEM_PROMPT = """
    Jesteś analitykiem Cybersec/IT. Analizujesz treść artykułu technicznego lub dokumentacji.
    Twoim zadaniem jest stworzenie zwięzłej notatki (tl;dr) oraz wyciągnięcie technicznych szczegółów.

    ZASADY:
    1. Skup się na faktach, konfiguracjach i kodzie.
    2. Formatuj w Markdown.
    3. Pomiń zbędne informacje (reklamy, wstępy).
    """

    def __init__(self, model: Optional[str] = None, gardener: Optional[ObsidianGardener] = None):
        self.vault_path = ProjectConfig.OBSIDIAN_VAULT
        self.model = model or ProjectConfig.OLLAMA_MODEL
        self.output_dir = self.vault_path / "Research"
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.gardener = gardener or ObsidianGardener()

    @staticmethod
    def clean_filename(title: str) -> str:
        title = title.lower()
        title = re.sub(r'[^\w\s-]', '', title)
        return re.sub(r'[\s_-]+', '-', title).strip('-')

    def fetch_article_content(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        logger.info(f"Fetching: {url}", extra={"tags": "WEB-RESEARCH"})
        headers = {'User-Agent': 'Mozilla/5.0 (Gemini CLI Bridge)'}
        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            
            for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
                script.extract()
                
            title = soup.title.string.strip() if soup.title else "Unknown Article"
            text = soup.get_text(separator='\n')
            
            clean_text = '\n'.join(line.strip() for line in text.splitlines() if line.strip())
            return title, clean_text
        except Exception as e:
            logger.error(f"Download error: {e}", extra={"tags": "WEB-ERROR"})
            return None, None

    def process_url(self, url: str) -> bool:
        title, text = self.fetch_article_content(url)
        if not text: return False

        safe_title = self.clean_filename(title)[:60]
        
        # Split into chunks if necessary
        chunks = [text[i:i+6000] for i in range(0, len(text), 5500)]
        full_notes = []
        
        for i, chunk in enumerate(tqdm(chunks, desc="AI Analysis")):
            try:
                resp = ollama.chat(model=self.model, messages=[
                    {'role': 'system', 'content': self.SYSTEM_PROMPT},
                    {'role': 'user', 'content': f"Fragment {i+1}:\n{chunk}"}
                ])
                full_notes.append(resp['message']['content'])
            except Exception as e:
                logger.error(f"AI Error: {e}")

        return self.save_note(safe_title, title, url, full_notes)

    def save_note(self, safe_title: str, original_title: str, url: str, notes_list: List[str]) -> bool:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
        filename = f"{datetime.now().strftime('%Y-%m-%d')}-web-{safe_title}.md"
        filepath = self.output_dir / filename
        
        joined_notes = "\n\n".join(notes_list)

        content = f"""
---
created: {timestamp}
tags: [research, web, ai-generated]
source: {url}
status: to-read
---

# Research: {original_title}

> **Source:** [{url}]({url})

---
## Analiza AI

{joined_notes}

---
*Generated by WebResearcher (Architect Edition)*
"""
        filepath.write_text(content, encoding='utf-8')
        logger.info(f"Research saved: {filepath}", extra={"tags": "NOTE-SAVE"})
        
        # Auto-link concepts
        self.gardener.process_file(str(filepath))
        return True
